{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Obtaining dependency information for bayesian-optimization from https://files.pythonhosted.org/packages/45/cf/3016b660afca02c6ecca3c1cc6d8df3b8f1a6ff4878103204d0aa6b4c769/bayesian_optimization-1.4.3-py3-none-any.whl.metadata\n",
      "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl.metadata (543 bytes)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from bayesian-optimization) (1.3.0)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (2.2.0)\n",
      "Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.3\n"
     ]
    }
   ],
   "source": [
    "! pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/e1/4c/4685ccfae9806f561de716e32549190c1f533dde5bcadaf83bdf23972cf0/lightgbm-4.3.0-py3-none-win_amd64.whl.metadata\n",
      "  Downloading lightgbm-4.3.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\casti\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\casti\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n",
      "Downloading lightgbm-4.3.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.3 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 0.9/1.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 10.6 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.3.0\n"
     ]
    }
   ],
   "source": [
    "! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Obtaining dependency information for catboost from https://files.pythonhosted.org/packages/e8/37/3afd3c02798734efcd7840bfa872d3efc06f5d5c92f9613fea3ff5b4311f/catboost-1.2.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading catboost-1.2.3-cp311-cp311-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting graphviz (from catboost)\n",
      "  Obtaining dependency information for graphviz from https://files.pythonhosted.org/packages/00/be/d59db2d1d52697c6adc9eacaf50e8965b6345cc143f671e1ed068818d5cf/graphviz-0.20.3-py3-none-any.whl.metadata\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (2.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (1.11.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (5.9.0)\n",
      "Requirement already satisfied: six in c:\\users\\casti\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\casti\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.2.2)\n",
      "Downloading catboost-1.2.3-cp311-cp311-win_amd64.whl (101.1 MB)\n",
      "   ---------------------------------------- 0.0/101.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/101.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/101.1 MB 435.7 kB/s eta 0:03:52\n",
      "   ---------------------------------------- 0.1/101.1 MB 1.2 MB/s eta 0:01:23\n",
      "   ---------------------------------------- 0.6/101.1 MB 3.5 MB/s eta 0:00:29\n",
      "    --------------------------------------- 2.0/101.1 MB 9.6 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 4.0/101.1 MB 16.1 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 5.1/101.1 MB 17.2 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 9.1/101.1 MB 24.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 10.6/101.1 MB 36.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 11.4/101.1 MB 34.4 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 13.8/101.1 MB 38.5 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 15.8/101.1 MB 43.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 18.5/101.1 MB 40.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 18.8/101.1 MB 31.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 22.7/101.1 MB 36.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 25.3/101.1 MB 38.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 28.6/101.1 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 31.7/101.1 MB 54.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 33.8/101.1 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 36.3/101.1 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 38.9/101.1 MB 43.5 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 40.9/101.1 MB 43.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 43.2/101.1 MB 46.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 45.2/101.1 MB 43.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 48.2/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 51.1/101.1 MB 40.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 54.3/101.1 MB 43.7 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 56.8/101.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 58.1/101.1 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 60.7/101.1 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 63.8/101.1 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 65.1/101.1 MB 38.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 67.9/101.1 MB 32.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 68.5/101.1 MB 32.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 69.2/101.1 MB 25.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 70.3/101.1 MB 25.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 71.8/101.1 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 74.6/101.1 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 76.3/101.1 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 77.0/101.1 MB 18.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.3/101.1 MB 19.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 78.8/101.1 MB 21.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 80.0/101.1 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 80.9/101.1 MB 24.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 81.8/101.1 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 82.6/101.1 MB 22.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 83.6/101.1 MB 20.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 84.5/101.1 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 85.1/101.1 MB 19.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 85.8/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 86.2/101.1 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 86.5/101.1 MB 14.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 88.3/101.1 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 88.3/101.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 88.3/101.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 88.8/101.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 89.0/101.1 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 89.3/101.1 MB 11.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 90.3/101.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.1/101.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.6/101.1 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 92.6/101.1 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.7/101.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 95.5/101.1 MB 11.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 96.4/101.1 MB 12.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 97.5/101.1 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.7/101.1 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.3/101.1 MB 17.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.3/101.1 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  100.8/101.1 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  101.0/101.1 MB 16.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 101.1/101.1 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.1/47.1 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2.3 graphviz-0.20.3\n"
     ]
    }
   ],
   "source": [
    "! pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'Bayesian_optimization_case_study.ipynb',\n",
       " 'flight_delays_test.csv.zip',\n",
       " 'flight_delays_train.csv.zip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    simple_func,\n",
    "    {'a': (1, 3),\n",
    "    'b': (4, 7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m6.302    \u001b[0m | \u001b[0m1.749    \u001b[0m | \u001b[0m4.553    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m6.313    \u001b[0m | \u001b[95m1.708    \u001b[0m | \u001b[95m4.604    \u001b[0m |\n",
      "| \u001b[95m3        \u001b[0m | \u001b[95m8.471    \u001b[0m | \u001b[95m1.522    \u001b[0m | \u001b[95m6.948    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m3.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m9.129    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m6.129    \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3.0, 'b': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two seperate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('flight_delays_train.csv.zip')\n",
    "test_df = pd.read_csv('flight_delays_test.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "def label_enc(df_column):\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "def feature_eng(df):\n",
    "    df['flight'] = df['Origin']+df['Dest']\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "        \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       #early_stopping_rounds=100,\n",
    "                       stratified=True,\n",
    "                       nfold=3)\n",
    "    print(cv_result)\n",
    "    #return cv_result['auc-mean'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 10 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "{'valid auc-mean': [0.6978983397465144, 0.7031100990182507, 0.7062372445493127, 0.7077203825044855, 0.709523316298259, 0.7106064248581244, 0.7121539581037233, 0.7130987780593946, 0.7137389169562249, 0.7146053151273667, 0.7151957104734835, 0.7158754398296421, 0.7163212663964154, 0.7167028687054003, 0.716912755922694, 0.7172352163823806, 0.7175436730192696, 0.7177843484571875, 0.7179844085139061, 0.7181344119563015, 0.7183196704403084, 0.7185873547786792, 0.7188118151346535, 0.719063399235128, 0.719118584203977, 0.7194107400925002, 0.7196071662230873, 0.7197727290735815, 0.7199954753179534, 0.7201862368859716, 0.720441995659197, 0.7205363070153966, 0.7206731754995616, 0.720870192181651, 0.7209689505894454, 0.7212288489677171, 0.7213271354805461, 0.7214794387311828, 0.7215999530098326, 0.7217529260842438, 0.7218092216951403, 0.7219129513657135, 0.7220574748522656, 0.722152441862924, 0.7222424332465455, 0.7223246316036963, 0.7224357325853057, 0.7225562925709804, 0.7226985842896259, 0.7227883413568111, 0.7228889392674587, 0.7230029426898875, 0.7231712528204741, 0.7232166834953434, 0.7234571974880639, 0.7235383584893356, 0.7236803035995648, 0.7239079621762784, 0.7240103035334254, 0.7240748634936583, 0.7242111900991149, 0.7243081328090692, 0.7243545715637484, 0.7244311044063806, 0.7245552671620162, 0.7246161506584841, 0.724720395454337, 0.7247661988470987, 0.7249163169644292, 0.7250558799546094, 0.7251410608063832, 0.7252643311048947, 0.7252920890227204, 0.7253608589098784, 0.7254252900902957, 0.7255059659313657, 0.7255753386045775, 0.7256451086081287, 0.7257738913608595, 0.7258469178210767, 0.7259057639147889, 0.7259813624968398, 0.7260696768649012, 0.726165714878562, 0.7262733300226634, 0.7263827970174418, 0.7265806988871041, 0.7266349229692971, 0.7267057941144198, 0.7267778230130436, 0.7268935646909659, 0.7270230174542238, 0.7271032337806048, 0.7271814397782732, 0.7272992053468071, 0.7274198119660832, 0.7274952191831522, 0.7275486724637136, 0.7276922348838384, 0.7278398304820248, 0.7280928921672875, 0.7281908056583237, 0.7282738377481528, 0.7284272709632873, 0.7285299212596178, 0.7286652502241927, 0.7288154364737357, 0.728908062517879, 0.7291084631885392, 0.7293055516682072, 0.7294319428221455, 0.7295604428813777, 0.7296455919107085, 0.7297767453140405, 0.7298681160061276, 0.7299826954463496, 0.7301041668502989, 0.7301862106099994, 0.7303362707139298, 0.7304166731972718, 0.7306056810782824, 0.7307327014446211, 0.7307977565077733, 0.7309674112404885, 0.7310649174175768, 0.7312500531054842, 0.7313557005295156, 0.7314608629131348, 0.7316096597819429, 0.7317061523551764, 0.7317852654398295, 0.7318995245573553, 0.7320352487770667, 0.7321505528251252, 0.732219142309848, 0.732306946331566, 0.732369722283949, 0.7324937913714044, 0.7326100771106754, 0.732681613833484, 0.7327572686825987, 0.7328163308411394, 0.7328880287599538, 0.7329723351431124, 0.7330660880685169, 0.7331263117620832, 0.7331929637147628, 0.7332899512826176, 0.7333718830773854, 0.7334812722361944, 0.7335585120762539, 0.7336157476913555, 0.7337166191761869, 0.7337708897573257, 0.7338429572569319, 0.7339117016626181, 0.7339766079794868, 0.7340588805278715, 0.7341214813735801, 0.7341882339954257, 0.7342464314992366, 0.7343209750282772, 0.7343916534970351, 0.734458341367123, 0.7345796480648965, 0.7346547793590421, 0.7347540068182082, 0.7348046088046813, 0.7348730109433831, 0.734951850655893, 0.7350127930764344, 0.735100649623243, 0.7351890590295743, 0.7352463344156385, 0.7353306776409845, 0.7353985289134165, 0.7354420134218963, 0.7354760555082058, 0.7355596462168386, 0.7355988349950996, 0.7356943256460076, 0.7357621625566728, 0.735815344481476, 0.7358766610558071, 0.7359180245574847, 0.7359699754017074, 0.7360257131341035, 0.7360760898066966, 0.7361293973471451, 0.736195251308108, 0.7362419116266364, 0.736298023439559, 0.7363666873226707, 0.7364437609749501, 0.7365135798468024, 0.7365396180458221, 0.7366246385149359, 0.7367217552571855, 0.7368070710459015, 0.7368879512717942, 0.7369408916794176, 0.7370021413689724, 0.7370640968230745, 0.7371322237478548, 0.7371658460449085, 0.7372031500033801, 0.7373039274483713, 0.7373323552044733, 0.737422257873987, 0.7375251864636807, 0.7375511272726216, 0.7375990422477251, 0.737648726580156, 0.7376898665113335, 0.7377365263612111, 0.7377990526986716, 0.7378474471696949, 0.7379017116257635, 0.7379209073894778, 0.7379576101924634, 0.7379838401157253, 0.7380078957835566, 0.7380733436775841, 0.7381373795083929, 0.7381782917563324, 0.738217059423568, 0.7382908799353141, 0.738321142369792, 0.7383523184731255, 0.7383828715661963, 0.7383925421967464, 0.7384176056829072, 0.7384471540475795, 0.7384736438698627, 0.7385257926443147, 0.738558709693138, 0.7385922854539638, 0.7386402829839209, 0.7386872380575337, 0.738737493865953, 0.7387730573959418, 0.7388176510776553, 0.738850147577256, 0.7388904455995502, 0.7389167217790017, 0.7389409739911276, 0.7389741873995236, 0.7390051220563301, 0.7390502406188121, 0.7390725133907793, 0.7391444706463716, 0.7391849074557748, 0.7391997052527696, 0.7392335316708287, 0.7392693129083286, 0.7392983501536139, 0.7393300765452336, 0.7393441977337556, 0.7393820574150082, 0.7394106948284436, 0.7394406480270797, 0.7394697143112497, 0.7395194504801753, 0.7395478324056378, 0.7395843104380674, 0.7396122796482761, 0.7396505761954416, 0.7397111086154974, 0.7397505990942316, 0.7397794200241399, 0.7398099551748675, 0.7398168650560449, 0.7398233019109677, 0.739855252852177, 0.7398610973166843, 0.7398847209947682, 0.7399228066925693, 0.7399658733487963, 0.7399946068486383, 0.7400090815547519, 0.7400175818952546, 0.7400280402174467, 0.7400481646968758, 0.7400741478854885, 0.740129044145133, 0.7401493611629205, 0.7401915144228557, 0.7402243341030008, 0.7402490097333795, 0.7402867507105081, 0.7403187874806488, 0.7403768985261566, 0.7403975471697133, 0.7404204440025156, 0.740439816373788, 0.7404746589401642, 0.7404863125304385, 0.740510639392121, 0.7405228542346078, 0.7405366192846947, 0.7405393003932695, 0.7405535509381432, 0.7405693979323559, 0.7405853715512151, 0.7406056742489938, 0.7406061445859259, 0.7406322725892779, 0.7406595255333945, 0.7407019731977528, 0.7407114201645832, 0.7407311668469664, 0.7407611822195573, 0.7407662221290009, 0.7407773736161684, 0.7407865535541474, 0.7408013813090909, 0.7408055855101621, 0.740836096513946, 0.7408547486479667, 0.7408741084242737, 0.7409038156393938, 0.74093252275418, 0.7409428596353873, 0.7409518155230472, 0.7409670491860401, 0.7409611576677126, 0.7409766197007972, 0.7409875715352005, 0.7409943010016783, 0.7410040368490215, 0.7410178624528431, 0.7410269596981668, 0.7410558523107785, 0.7411040567775332, 0.7411038429483018, 0.7411146249108572, 0.7411357597518884, 0.7411660921947748, 0.7411926087206692, 0.741193584497565, 0.7412408287667699, 0.7412544018665993, 0.7412891292796101, 0.7413121483156145, 0.741339692106631, 0.7413567159344403, 0.7413836821945967, 0.741414426098817, 0.741426249969777, 0.7414295252187975, 0.7414281130116707, 0.7414371223021368, 0.7414534003463432, 0.741459208606679, 0.7414789499289073, 0.7414841048064371, 0.7414927500730549, 0.7415142694911482, 0.7415314997067693, 0.741566669844541, 0.7415885668271538, 0.7415870121008087, 0.7416067451671626, 0.7416003761964823, 0.7416245328538867, 0.7416314263166925, 0.7416549128278845, 0.7416731022313701, 0.7416940699032176, 0.7417017116681816, 0.741723653566401, 0.74174042012874, 0.7417591196228678, 0.7417710882831346, 0.741795897878998, 0.7418235889242967, 0.7418487534216768, 0.7418539747393376, 0.7418791692171096, 0.7418896670226508, 0.7419025090635355, 0.7419272278271322, 0.7419369635150249, 0.7419436874638095, 0.7419584216022805, 0.7419751705022076, 0.7419830263606525, 0.7419835763151577, 0.7419749290075645, 0.7419823514614204, 0.74199127869593, 0.7420062351665911, 0.7420216335009547, 0.7420257729405755, 0.7420247571031896, 0.7420181082997471, 0.7420206002489457, 0.7420227765775022, 0.7420406083927431, 0.7420714774800565, 0.7420923398248763, 0.742112853863607, 0.7421285596594741, 0.7421267424382845, 0.7421424384354381, 0.7421521145652209, 0.7421534123071512, 0.7421721351196254, 0.7421797465231995, 0.7421816729022387, 0.74220146064332, 0.7422102407005348, 0.7422367599605594, 0.7422427990319997, 0.7422589240218298, 0.7422637897299772, 0.7422747983290875, 0.7422768577316569, 0.7422961510834926, 0.7423033608455709, 0.7423108050578082, 0.7423109524889853, 0.7423256952724987, 0.7423399032047739, 0.7423490318604125, 0.7423665441902801, 0.7423853117562671, 0.7423972895298236, 0.7423999155253848, 0.7424310519011096, 0.7424305001658795, 0.7424375688966155, 0.7424411852328127, 0.7424575796088603, 0.7424761312692093, 0.7424892582363024, 0.7425030025297085, 0.7425107853748093, 0.7425025332712519, 0.7425117913380794, 0.7425231966114435, 0.7425486117211667, 0.7425466531815892, 0.7425409996572127, 0.7425574488969545, 0.742572027045146, 0.7425630004773648, 0.742574635229437, 0.742594825953637, 0.7425956799494124, 0.7426079276040632, 0.7426141126759384, 0.7426131980164307, 0.7426121380124447, 0.7426144943377553, 0.7426265689042472, 0.7426391224552976, 0.7426470990988578, 0.7426579717637555, 0.742662638784498, 0.7426675968527511, 0.7426775794027564, 0.7426666396221213, 0.7426758796118342, 0.742682314280537, 0.7427044201482543, 0.7426922470209277, 0.7426908322178155, 0.7427102572990125, 0.74270290899811, 0.742716462492932, 0.742707842461653, 0.7427013439019686, 0.7427123616932348, 0.7427335497343984, 0.7427560055516462, 0.7427614859612559, 0.7427472738819558, 0.7427612629372741, 0.7427621961606814, 0.7427650087012148, 0.7427720665222514, 0.7427868803019096, 0.7427977503694504, 0.7428072386846339, 0.7428091909284017, 0.7428145992824957, 0.7428231263875166, 0.7428211729140443, 0.7428323545242566, 0.7428274791713116, 0.742827427589532, 0.7428200519314164, 0.7428283340170257, 0.7428202058365653, 0.7428345202027958, 0.7428304492491485, 0.7428369697731255, 0.7428409449363521, 0.7428628328550788, 0.7428661581717729, 0.7428681346824236, 0.7428856339272906, 0.742881191156256, 0.7428807609417866, 0.7429119187012764, 0.7429435993987349, 0.7429520834275506, 0.7429768250887383, 0.7429605152758939, 0.7429667251620743, 0.7429683781783245, 0.7429727016794639, 0.742981195034181, 0.7429803436269812, 0.7429780475344296, 0.7429948657629026, 0.7429959449984983, 0.7430128013739127, 0.7430122211474243, 0.7430252437284285, 0.743034208500243, 0.7430502384993195, 0.7430554959103556, 0.743058513598989, 0.7430672709547803, 0.743061794131462, 0.7430736730326403, 0.7430728026860338, 0.7430784024738161, 0.7430903761998474, 0.743085084039904, 0.7430875063592531, 0.7431055291009413, 0.7431379170372067, 0.7431405675297821, 0.7431297789418111, 0.743136444376899, 0.7431566718467915, 0.7431627069643785, 0.7431721030455761, 0.7431599862815723, 0.7431646689881789, 0.7431753844889553, 0.7431883443598942, 0.7431843359536391, 0.7431944609837156, 0.7431875784518404, 0.7431884547666785, 0.7431927413759274, 0.7432057172566577, 0.743212097005745, 0.7432350571107073, 0.7432411518754066, 0.7432558196866766, 0.7432490593007696, 0.7432540651394239, 0.7432514809103536, 0.7432500095430541, 0.7432441361979144, 0.7432515735389176, 0.7432528515785606, 0.7432679884213137, 0.7432769531972904, 0.7432818776690722, 0.7432680932740539, 0.7432651515803119, 0.7432967580196479, 0.7433068131762449, 0.7433090973041799, 0.7433114091391283, 0.7433254746253134, 0.7433287157304854, 0.7433502979376317, 0.7433608598351542, 0.743349489452938, 0.743348421290717, 0.7433481492470252, 0.7433431770998847, 0.7433284529780456, 0.7433320485960758, 0.7433363046413174, 0.7433536286790035, 0.7433616815303088, 0.7433871209322865, 0.7433935599613034, 0.7433853956774548, 0.7433924134764841, 0.7434047341943559, 0.7434055075937884, 0.7434088315311987, 0.7434098185910017, 0.7434170285382485, 0.7434215131616835, 0.7434235547670118, 0.7434317542207456, 0.7434383128937309, 0.7434473031882917, 0.7434422322036413, 0.7434378012359328, 0.743445793394142, 0.7434420357103017, 0.7434489885547241, 0.743450256955111, 0.743453745124255, 0.7434506072857393, 0.7434566876455957, 0.7434648794402984, 0.7434831959589584, 0.7434861677412918, 0.7434878132915514, 0.7434914917329181, 0.7435028072440417, 0.7435154571055822, 0.7435087096381207, 0.7434997095942076, 0.7435004709106107, 0.7434870337102683, 0.7434839873101174, 0.7434910227789536, 0.7434876907393427, 0.7435023828346764, 0.7435088347275944, 0.743494929060143, 0.743490239282521, 0.7434843058620716, 0.7434925835190218, 0.743504559148712, 0.7435072155454959, 0.7435261611859717, 0.7435384130336952, 0.7435458013509719, 0.7435416537039182, 0.7435425975576552, 0.7435608661429449, 0.7435522509435267, 0.7435545334201278, 0.7435478672440582, 0.7435602053369271, 0.7435490140934012, 0.7435532041209729, 0.7435628723579338, 0.7435577172118389, 0.7435657842391165, 0.7435736971830152, 0.743572219253831, 0.7435775014128098, 0.7435766852825189, 0.7435955391268368, 0.7435976554050446, 0.7436001488053091, 0.7436096234056198, 0.7436054928901195, 0.7436083941890752, 0.7436201636166632, 0.7436140561269378, 0.7436079812076862, 0.743610391334696, 0.7436181835783735, 0.743633282429515, 0.743629723854846, 0.7436375182078008, 0.7436465007538677, 0.7436466228320153, 0.7436426109411928, 0.7436451267651787, 0.7436354219365002, 0.7436394394528424, 0.7436508114808329, 0.743663028410134, 0.7436721703420451, 0.7436828333825461, 0.7436898366254692, 0.7436900373103604, 0.7436900327380039, 0.7436894751061383, 0.7436871813003257, 0.7436883964933982, 0.7436878993850975, 0.7437025819919588, 0.74369920080587, 0.7436939768307004, 0.7436962567508538, 0.7436792151196112, 0.7436965885518624, 0.7436893201723395, 0.7436974966941201, 0.7436824769189961, 0.7436917537098914, 0.7436923845404627, 0.7436882500556239, 0.743688037417955, 0.7437046937141245, 0.7437162837981205, 0.7437190007846471, 0.7437233835766147, 0.7437222512900195, 0.7437228133996885, 0.7437351716019928, 0.7437401441426509, 0.7437386812057079, 0.7437343652076818, 0.7437394892313325, 0.7437358688646567, 0.7437363341207858, 0.7437479969075929, 0.743743767422397, 0.7437453376954117, 0.7437452594937083, 0.7437555366479689, 0.743760627192287, 0.7437591009510407, 0.7437604812721972, 0.7437626066241947, 0.7437657324222878, 0.7437624136985509, 0.743763244139581, 0.7437615891849819, 0.7437599549465682, 0.7437808992268157, 0.743783260903783, 0.7437843846345804, 0.7437863199072366, 0.7437865229871231, 0.7437862971144171, 0.7437872187963622, 0.7437919978676663, 0.7437957391133545, 0.7438060631799374, 0.7438104952108735, 0.7438117235388759, 0.7438322624604918, 0.7438242642113132, 0.7438233247447638, 0.74382514967135, 0.743814168343463, 0.7438208019263556, 0.7438302536879614, 0.7438203586824984, 0.7438387206107198, 0.7438496072155885, 0.7438579253540863, 0.7438719484794659, 0.7438822825267709, 0.7439019424316348, 0.7439063501413186, 0.7439098637369729, 0.7439119830031187, 0.7439204218381331, 0.7439089861427236, 0.7439279517683858, 0.7439297205119675, 0.7439299703308017, 0.7439285121629756, 0.7439252944275717, 0.7439441801989924, 0.7439491424123169, 0.743951874689183, 0.7439593373593181, 0.7439605987933495, 0.7439438482982813, 0.7439339129148038, 0.7439454501956009, 0.7439514311710843, 0.7439561705451189, 0.7439465186159425, 0.743948106153939, 0.7439573692154573, 0.7439520228397214, 0.743946195588916, 0.7439360616612536, 0.7439371425052421, 0.7439385213240493, 0.7439290029649479, 0.743931101348209, 0.7439248925711528, 0.7439373146191274, 0.7439411785077134, 0.7439391786870081, 0.7439433216357582, 0.7439500063551, 0.7439607493196773, 0.743945308342388, 0.7439413659311541, 0.7439400895928826, 0.7439417963120505, 0.7439413323298422, 0.7439325822391293, 0.7439330943780317, 0.743930820299361, 0.7439377144482607, 0.7439192273535014, 0.7439168794966878, 0.7439233291833117, 0.7439413696054613, 0.7439234422939736, 0.7439175842093534, 0.7439217654414224, 0.7439161619694113, 0.7439045582939832, 0.7438987475217668, 0.7438860121910564, 0.743871679328653, 0.7438848084520818, 0.7438780078320791, 0.7438733283614655, 0.743883415443737, 0.7438957863439484, 0.74390107616675, 0.7439047279814162, 0.7439050897791643, 0.7439108900108723, 0.7439087942508591, 0.7439134864636868, 0.7439197865837102, 0.743927019373118, 0.743924793007071, 0.7439330944571392, 0.7439137403696116, 0.7439025880630433, 0.743903957241184, 0.7439120264936396, 0.7439094302014545, 0.7439184943117239, 0.743905069321413, 0.743900394102529, 0.7439013738489851, 0.743904307034167, 0.7439095738471458, 0.7439072786151503, 0.7439069901770123, 0.7438844482963504, 0.7438903220155062, 0.7438887132838552, 0.7438800925411937, 0.7438656542758401, 0.7438667953327524, 0.7438858269222116, 0.7438749455218926, 0.7438722473588069, 0.7438606114345228, 0.7438632443557124, 0.7438640731789538, 0.7438693122343144, 0.7438884450669335, 0.7438956919222024, 0.7438995816309965, 0.743902876710481, 0.7439038562539774, 0.7439050244078237, 0.7439100572946437, 0.7439045301968532, 0.7439102513556572, 0.7439059252476184, 0.7439055645830447, 0.7439030146698338, 0.7438942017990889, 0.743900943162072, 0.7438996986809004, 0.7438841012762613, 0.7438837681325837, 0.7438914180852789, 0.7438966826530468, 0.7438883275653593, 0.7438881668954599, 0.7438805068075521, 0.743886390945872, 0.7438872904586207, 0.7438789062570151, 0.7438836277721356, 0.743892796735777, 0.7438965620299061, 0.7438968013762223, 0.7438931656352926, 0.7438898843140428, 0.743887582571908, 0.7438796879226167, 0.7438817955454554, 0.7438731554815736, 0.7438743382694075, 0.7438566144144589, 0.7438649704607982, 0.7438740094511168, 0.7438606927726074, 0.7438741568480024, 0.7438723829416306, 0.7438667779313851, 0.7438633777818583, 0.7438631229135068, 0.7438669352450908, 0.7438615176186484, 0.7438566038051607, 0.743875063503665, 0.7438641294735954, 0.7438620930567074, 0.7438670804787807, 0.7438650435658277, 0.7438685141700908, 0.7438629671589249, 0.7438620559420794, 0.74386215129825, 0.7438478485238752, 0.7438505850809749, 0.7438352062801084, 0.7438300295106967, 0.7438313885632204, 0.7438338046485056, 0.7438323015924425, 0.7438275914021769, 0.7438209198445316, 0.7438170888721688, 0.7438135411849432, 0.7438196271902188, 0.7438267001606214, 0.7438176554934838, 0.743813206542712, 0.7438205736344224, 0.7438294106526163, 0.7438208543178716, 0.7438227164641896, 0.7438221961127564, 0.7438146706982751, 0.7438159369805536, 0.7438149208237784, 0.7438141957707846, 0.7438066117344823, 0.7438014039680046, 0.7437967428827409, 0.7437931024894983, 0.7437811154549578, 0.7437706488731983, 0.7437726530871073, 0.7437661521868114, 0.7437851993109645, 0.7437790309125636, 0.7437785931638428, 0.7437649377485993, 0.7437596440401889, 0.743751759929412, 0.7437538284365638, 0.7437440862963945, 0.7437518170243113, 0.7437492385759166, 0.7437407686142276, 0.7437502396789483, 0.7437517886702083, 0.7437484033941558, 0.7437496869793252, 0.7437651503449225, 0.7437600067691329, 0.7437602679907372, 0.743757852768448, 0.7437616566537439, 0.743778863157117, 0.7437783100644757, 0.7437897047239708, 0.7437832208587558, 0.7437748193212651, 0.743765453043966, 0.743764048579797, 0.7437624676571649, 0.743764003532798, 0.7437671125524772, 0.7437651566937084, 0.7437677880287191, 0.7437620665016434, 0.7437620464437987, 0.7437593063899351, 0.7437643933329795, 0.743758477781968, 0.7437639651248928, 0.7437712977469992, 0.7437796630903485, 0.7437807653868441, 0.7437826792139134, 0.7437918097180263, 0.7437874632286731, 0.7437827436228477, 0.7437640393442698, 0.7437481624087207, 0.743743939877921, 0.7437362686342177, 0.7437209379824011, 0.7437200825610667, 0.7437227179919348, 0.7437224643167779, 0.7437340662624373, 0.7437346349445066, 0.7437353323003896, 0.743728301357932, 0.7437280033633425, 0.743728332284903, 0.7437186597155815, 0.7437117454454302, 0.743708461944219, 0.7437120704172461, 0.7437089395469255, 0.7437106376336109, 0.7437053215213701, 0.7437033834691683], 'valid auc-stdv': [0.002293949030532518, 0.0037865040017041047, 0.0038376329683094254, 0.0037238806423796774, 0.004127369786187555, 0.0041509395707763075, 0.0039344712352752526, 0.0036743146625062926, 0.0032626052720547078, 0.0033704876847952996, 0.003163392266001953, 0.003040259337741428, 0.0029185135346755054, 0.002847470733617102, 0.002696547829255572, 0.002822286576598178, 0.002676983988779944, 0.002712656727407751, 0.0025613545438970932, 0.0024921288788005763, 0.0024610147307043417, 0.0024523000298798457, 0.002385866465487795, 0.0022704601804895684, 0.002206643132770673, 0.0020784331915321337, 0.00197969033335828, 0.0019046745932598333, 0.001902734428970018, 0.0018368652753281997, 0.0018247660169517356, 0.0017654384697661156, 0.001872530712972092, 0.0018111399594325061, 0.0017592343547300387, 0.0016384446176549107, 0.0016482982312574083, 0.0016179245992807457, 0.0016189282839933806, 0.001604221639644766, 0.0015597734010649586, 0.0014809995639959914, 0.0015765089849045367, 0.0014216368181903475, 0.0013953908361577156, 0.00136356840046265, 0.0013055172631282685, 0.0013096316750605289, 0.0013640131784625324, 0.0013472959656977296, 0.0013405247955523628, 0.0013854438795240257, 0.0013411143950695088, 0.0013392584798508566, 0.0014229917600872915, 0.0013825622752291365, 0.0013122736989276238, 0.0013732011111815114, 0.0013378959259783406, 0.0013365095577885955, 0.0012815216154233437, 0.00123414318314465, 0.001185537436181545, 0.0011559895725918726, 0.001172483161366874, 0.001153366989739151, 0.001084030161438909, 0.0010555171107569088, 0.0009769690561731777, 0.0009740626484376123, 0.0009067461919347146, 0.0008816858848312368, 0.0008485371536270488, 0.0008603184703003479, 0.000829197016807646, 0.0008471948408507465, 0.0009036394876628362, 0.0008515380373808323, 0.0008897507215776276, 0.0009286749849510715, 0.0008991913578211758, 0.0009371140110129693, 0.0009401833469268691, 0.0008749935556903542, 0.000854036114092244, 0.0008785874894307114, 0.0007929584401346606, 0.0008302414148740978, 0.0008515328890483189, 0.0008461491993829428, 0.0008984130961419088, 0.0008464810262124248, 0.00085798863197625, 0.0008215902058628668, 0.000829012875519131, 0.0009005524717724614, 0.0008846230873564921, 0.0008168304402051613, 0.0007802539166780618, 0.0007158939314980416, 0.0007539956479053829, 0.0008027499334542439, 0.0007634869166557783, 0.0007793061970575088, 0.0007903387873573058, 0.0007918941775307014, 0.0008018305793978548, 0.0008126920764863785, 0.0008439432277315696, 0.0008933753011479274, 0.000849595979399909, 0.0008635910744574626, 0.0009133793974377518, 0.0008722137523313632, 0.0009088130514109463, 0.0008960277371319371, 0.000987327743508142, 0.000998888598946514, 0.0010307385637744787, 0.0010519053772309525, 0.0010548101255195893, 0.001070419879289238, 0.0010685828248506624, 0.0010578699193375687, 0.0010449972629889947, 0.0009939865902554002, 0.001057348314347398, 0.0010968132434570847, 0.0011055601242855179, 0.0011143496624907855, 0.0011080703083155379, 0.001106623162751701, 0.0011023930524851552, 0.0010946711349533783, 0.0011086143370178554, 0.001110372179841935, 0.0010940024874677209, 0.0010950528928167181, 0.0010850979131489962, 0.0011064171126983528, 0.0011275332981339427, 0.0011223228470193835, 0.0011393610198988935, 0.0011493771352678829, 0.0011343362791646815, 0.00113141328968792, 0.001139782618250481, 0.0011870352426559313, 0.0011917355791939343, 0.001207762889602856, 0.00118550733833962, 0.0012276022624693765, 0.0012208930575749572, 0.0012280835911042164, 0.0012526091915927315, 0.0012460871436525483, 0.001246558242075373, 0.001267548245651844, 0.0013056475618961909, 0.0013036131986714787, 0.0013214310327036613, 0.001308694033337076, 0.0013383631141798927, 0.001389748119824161, 0.0013968095001980951, 0.001407265829139216, 0.0014305966765086366, 0.0014164325068242013, 0.0014138312278893055, 0.0014112878653593567, 0.0014003180886549879, 0.0014085673749284697, 0.001427061937157172, 0.0014291397000977478, 0.0014358020555843112, 0.0014383428564950377, 0.0014455848319538391, 0.0014681691628204271, 0.0015055251597190743, 0.001496119739348396, 0.0014751616617893523, 0.0015034148885375266, 0.0015222647951412013, 0.0015215774251435177, 0.001519205676796371, 0.0015440010960850363, 0.0015542266283729636, 0.001589137066594982, 0.0015872019691293778, 0.001560984473298514, 0.0015464338901259335, 0.0015625784719068422, 0.0015424477811630517, 0.001534343541246102, 0.0015101577361063443, 0.0015004014666029752, 0.0014873704465819069, 0.0015012766710184122, 0.0015468139298466168, 0.0015195617202772712, 0.0015259799588323902, 0.0015319811276832036, 0.0015712218251174494, 0.0015545440098611281, 0.0015441019368782074, 0.0015454570209678335, 0.0015280153060073486, 0.0015289937220043367, 0.001522624987233423, 0.0015211934869829734, 0.0015012580421270565, 0.00151659516568413, 0.001520215685657516, 0.0015276110775929044, 0.0015329853140511878, 0.0015369450974914976, 0.0015489638830957543, 0.001541569269689109, 0.0015388071124963192, 0.0015360762128116758, 0.0015561603626048192, 0.0015402538790496277, 0.0015430879321856733, 0.001555557565834275, 0.0015428489794057215, 0.0015510361522072917, 0.0015657549769765456, 0.0015767085088081371, 0.001569138802306413, 0.0015581082609279467, 0.0015692081304289369, 0.0015830178174149974, 0.0015810304505073838, 0.001614481678681579, 0.0015975547156361928, 0.0015978064713329152, 0.0016057837545158204, 0.001607292667708309, 0.0016135171760579834, 0.001647297617473705, 0.0016531447546635923, 0.0016724204518645714, 0.0016846830438710491, 0.001700742379101803, 0.001729934298472241, 0.0017554605307712606, 0.0017428700455648695, 0.0017654091580803447, 0.0017489221806425773, 0.0017444141629794087, 0.001760949787763022, 0.001747203921173823, 0.0017504040925437887, 0.0017487085710596862, 0.0017598743448497564, 0.0017857809718831317, 0.0017774005748321363, 0.0017659655996250733, 0.0017715386005657757, 0.0017702550702235363, 0.0017866310518898487, 0.0017959740482527712, 0.0017798509561019803, 0.0018014019461027448, 0.0018184065480848783, 0.0018205275099348129, 0.0018087417022454716, 0.0017694594966438458, 0.0017750123676372417, 0.0017903467914556377, 0.0018004985221867407, 0.0018141563107366898, 0.0018292810828868036, 0.001827347717568244, 0.0018458423685163046, 0.0018411738667714185, 0.0018298998800808943, 0.001814245348650175, 0.0018157688938514487, 0.00180751204718279, 0.0018160169568036144, 0.0018153195816721184, 0.0018176718228879079, 0.0018190189354806625, 0.0018109870470649242, 0.001804903832038094, 0.001815195038050413, 0.001801786916930191, 0.0017980855314358008, 0.0017833339712702739, 0.0017710984454769314, 0.0017542816798602166, 0.001746695920326671, 0.0017474041428573227, 0.0017610324826525347, 0.0017634329924023108, 0.0017533363961915723, 0.0017526206350940037, 0.0017456878305680938, 0.0017461897314958684, 0.0017470270273098974, 0.001716959564982771, 0.0017329891951176786, 0.0017238726135690406, 0.0017163426587050125, 0.0017043455781169039, 0.0016927296185835921, 0.0016735561549426536, 0.0016874594072860752, 0.0016840884824662465, 0.0016683290417711994, 0.0016951529591251206, 0.0016894204289024904, 0.0016807098346998451, 0.0016954602483180983, 0.0016894272056480677, 0.0016982874437489388, 0.0016823289898347667, 0.0016781408658229427, 0.001681717211199411, 0.0016862191070634717, 0.0016781580526627818, 0.001696586139162513, 0.0017063898535155137, 0.0017046617609867407, 0.0016847901864761041, 0.0016819464535732936, 0.0016797738366137228, 0.0016765310926159212, 0.0016774223401288672, 0.0016818627024862871, 0.0016768733012781658, 0.0016799338503284168, 0.0016601411087387224, 0.001672275635822563, 0.0016707602226903554, 0.0016631672745198668, 0.001646259225122561, 0.0016411129688829386, 0.0016436653193820168, 0.0016482183600756144, 0.0016420969939109937, 0.0016210020483502507, 0.0016271852055337887, 0.0016225107958205082, 0.0016155409378836591, 0.0015909140136219266, 0.0016167073684944413, 0.001616218289041245, 0.0016120133349937166, 0.001610654269240818, 0.0015966882480393015, 0.001594035066455475, 0.001586626086782025, 0.001589468906244265, 0.001580052439103897, 0.0015697530070052405, 0.0015760473503637173, 0.0015692125794405471, 0.001547556677281012, 0.0015464080376026191, 0.0015445958972428288, 0.0015441916608073306, 0.00154351273856484, 0.0015550341709072372, 0.0015329119208859938, 0.0015305829796884035, 0.0015440359300496103, 0.0015456064139217295, 0.0015292214676037186, 0.001526776080096824, 0.0015386133709864801, 0.001547244657359954, 0.0015593367685370312, 0.0015706662850644643, 0.0015747307516360495, 0.001594323517439828, 0.00158754365500633, 0.0016004123812596292, 0.001596158450382348, 0.0015867007981336255, 0.0015828979789391448, 0.0016126916290865579, 0.0016039915030935656, 0.0015968955416914266, 0.0016080161008598173, 0.0016003756745366362, 0.0016113867161573717, 0.001611828319296445, 0.0016170684613369098, 0.0016335519812461178, 0.0016286590559729992, 0.0016416344383022629, 0.001642503192019867, 0.001660022537796029, 0.0016480899079903066, 0.0016331618101933377, 0.0016514718014822795, 0.0016462876574627819, 0.001655471414957162, 0.0016389966056872072, 0.0016438147641970543, 0.001635892819846988, 0.0016382480339287647, 0.001630259460535678, 0.0016398949944867566, 0.0016609086415931265, 0.0016682994931067577, 0.0016834502495422137, 0.0016842380253461917, 0.0016854184726021824, 0.0016749473118877894, 0.0016818223175599288, 0.0016761734971753883, 0.0016664180858812396, 0.0016591685852570658, 0.0016479345745826013, 0.0016555373655306174, 0.001656651283845912, 0.0016635414494471001, 0.0016777943766358961, 0.0016661559739304832, 0.0016524366746584372, 0.0016684461525027243, 0.0016659927502051435, 0.0016630059633850572, 0.0016753708765234378, 0.001681819346128015, 0.0016797516754891416, 0.0016805840585018814, 0.0016837957673990428, 0.001681219017608167, 0.001670759956519467, 0.001671829239123203, 0.001673164888834066, 0.00167774166139507, 0.001684265805171098, 0.0016829333676289097, 0.0016736436608721754, 0.0016605666133317134, 0.001660795108556945, 0.0016586069768686233, 0.001661271079921344, 0.0016760632917542423, 0.0016513839363396184, 0.0016572312800707508, 0.0016351938127888853, 0.0016320885945655837, 0.0016240830880666184, 0.0016313664261775624, 0.0016423043547978476, 0.0016328092477299312, 0.0016349976145797495, 0.0016321407592169615, 0.0016212619425110325, 0.001610342239213417, 0.0016078013778858913, 0.0015929088002734062, 0.0015838800173701708, 0.0015999411772349835, 0.0015985279254254958, 0.0015963338470130766, 0.0016027463794214037, 0.0016017913753896902, 0.001595223615864143, 0.0016011238797356223, 0.0016005135137898523, 0.0016002897088940639, 0.0015961949248267976, 0.0015952391340621893, 0.0015900202574274725, 0.0015916746906532201, 0.0015846080435802114, 0.0015827859017447203, 0.0015816957253434079, 0.0015725044651259887, 0.0015610715873709098, 0.0015545902962859595, 0.0015615502161743875, 0.0015646213816866922, 0.001574645212774383, 0.0015873481257936359, 0.0015915887309684, 0.0015859343551411376, 0.0015880604101110114, 0.0015892232720633949, 0.0015887217824955308, 0.0015951310936247473, 0.0016087895131250973, 0.0015959592799096875, 0.001589579064096882, 0.0015749360966497436, 0.0015696428802175104, 0.0015813553336882777, 0.0015670708295938529, 0.0015647966800015779, 0.0015567281406756463, 0.001562381205769257, 0.0015583066259084223, 0.00154774664017753, 0.0015348969064801492, 0.0015331053122466564, 0.001536400259668506, 0.0015323330752507996, 0.0015440425721868624, 0.0015605963402048853, 0.0015608996652345065, 0.0015595613943146685, 0.0015647894422910228, 0.0015567459384923017, 0.0015561141253113856, 0.0015479261213620528, 0.00154926202852139, 0.0015634076684421833, 0.0015624396776094474, 0.0015614240708541397, 0.0015795968946298537, 0.001587287165860597, 0.0015783726039319992, 0.0015808938059715318, 0.0015714735966882898, 0.0015549869300062946, 0.0015422538713168949, 0.0015248320976642214, 0.0015151476675501316, 0.0015264898031252289, 0.0015239654622664987, 0.0015416800869047116, 0.0015317410512378244, 0.001534067709667283, 0.001522377959049642, 0.001515489891207942, 0.0015202903388013085, 0.001506295500196348, 0.0015285006281547788, 0.0015351027531011017, 0.0015209908833345239, 0.0015241224754245191, 0.0015310136388152105, 0.001535130747811752, 0.0015426150065237121, 0.0015382552549834875, 0.0015412733414949635, 0.0015440890699834834, 0.00155502228198114, 0.0015295850360734875, 0.0015270073968687208, 0.0015345524879164757, 0.0015353086294142041, 0.0015280307798639283, 0.001513611617842755, 0.0015048192097817268, 0.0015182122206820515, 0.0015101162284906332, 0.001502108644698567, 0.0015123458477390933, 0.0015335084463443773, 0.0015280193008930713, 0.0015233939729817317, 0.001535407311200806, 0.001523084173301387, 0.0015170232778212256, 0.0015221194837407657, 0.0015209207183764812, 0.0015122681147182024, 0.0014971807758051153, 0.0014888351395093938, 0.0014828806124280954, 0.0014825398737740403, 0.0014835677592952813, 0.0014740513974037233, 0.0014720040547834472, 0.0014774979013288713, 0.00147256376009331, 0.001460306832369976, 0.0014401206103098589, 0.0014303905077617729, 0.001406941223145705, 0.0014036536574469401, 0.0013991746918944636, 0.0014109039417787178, 0.0014165770519883282, 0.0014148230612804778, 0.0014106711033481276, 0.00141993007238127, 0.0014202487229491283, 0.001416034640129945, 0.0014158048644091522, 0.00141502961560917, 0.0014217842423130465, 0.0014198889229306176, 0.0014202510727475079, 0.0014171641034980156, 0.001412839532108522, 0.001420656646796624, 0.00140874974560669, 0.0014072600610539466, 0.001398907341630946, 0.001400972902908208, 0.0013847791805920683, 0.0013997255190939797, 0.0013904241324930139, 0.001396271021911908, 0.001409330846524428, 0.0014068394362349906, 0.0014208892526587802, 0.0014248388766161212, 0.001425669837498383, 0.0014200486315966109, 0.0014265343375869561, 0.0014286437016047214, 0.0014340440263106206, 0.0014346395422249897, 0.0014557827116934052, 0.0014456653744901139, 0.0014471741925782217, 0.0014292659701334875, 0.001440520072592805, 0.0014518042938581207, 0.0014690202953628788, 0.0014760498217418103, 0.0014662038791988324, 0.001474572998485389, 0.0014718884262132077, 0.0014758657843147766, 0.00149283087158021, 0.0014959648219523456, 0.0015011957086289037, 0.0015062357302675132, 0.0015181366799566405, 0.0015147717796292844, 0.0015109056654125682, 0.0015284077343997158, 0.0015277718923913546, 0.0015297384635457342, 0.0015263259500008168, 0.0015155702766243902, 0.0015205399421431504, 0.0015166922393683477, 0.001523501195796467, 0.0015179588185375815, 0.001517435376686179, 0.0015209019225607145, 0.0015267265007207198, 0.0015235001352887504, 0.0015185626865280642, 0.0015189290977421818, 0.0015136934552586287, 0.0015084452566291155, 0.0015105120245926518, 0.0015132662207096403, 0.0015132361154835722, 0.0015062466731696818, 0.0015027865544342326, 0.0015087515205766718, 0.00151061333171456, 0.001507806629102733, 0.0015017087197175534, 0.0015037524187338453, 0.0015114094782860427, 0.0015198942931368092, 0.0015251011473460569, 0.0015372195109388971, 0.0015397950413592494, 0.0015259908729337586, 0.0015153861644020708, 0.0015205120072915137, 0.0015143722306758876, 0.0015208327583149197, 0.0015181863989351862, 0.0015176153170345713, 0.0015111641613760436, 0.001521279281823957, 0.001520719634321209, 0.0015108912171466512, 0.0014990463528792221, 0.0015012761534848186, 0.0015070566141988826, 0.0015101180920720012, 0.0015148755532517326, 0.0015133388318791373, 0.0015290551486425304, 0.0015289550086247622, 0.0015226789887189418, 0.001522282108002473, 0.0015281167581522842, 0.0015301060231787626, 0.001525887578083368, 0.0015357038939020217, 0.0015455809820621401, 0.0015489805226403448, 0.001533801258186234, 0.0015387662480678042, 0.0015528674147911605, 0.0015400057356883597, 0.001553144424542645, 0.0015403132298663787, 0.0015428110701803009, 0.0015469318870878913, 0.0015538204551186101, 0.0015358592468014325, 0.0015361563283574017, 0.00154090049454985, 0.0015420802038874266, 0.0015484076448716126, 0.0015593131650954949, 0.001560497486697904, 0.0015605174738383952, 0.0015554254769377962, 0.0015639199273730332, 0.0015580111043704931, 0.0015590858254863075, 0.0015611898415413559, 0.0015712566209445764, 0.0015666269833883569, 0.0015673901050439493, 0.001570896795121768, 0.0015723221748281165, 0.0015697062492257335, 0.0015488353175561586, 0.0015479535825118137, 0.001540555633906039, 0.0015414782509069458, 0.001550284659052012, 0.00154346334121318, 0.0015433946340132996, 0.0015399479732362503, 0.0015346374593915367, 0.0015250377498801885, 0.0015098235062819562, 0.0015066277645277573, 0.0014977339859448711, 0.0015065639643608041, 0.0015061650012947775, 0.0015056333513297651, 0.0014977515355259021, 0.0015012819304199908, 0.0015045842538753107, 0.0014939870432368774, 0.0014924285957123343, 0.001482793504047576, 0.0014836243029517436, 0.0014891740794959532, 0.0014885424276145714, 0.0014827967669154751, 0.0014752096951458303, 0.0014772129224754384, 0.0014616411526844643, 0.0014713903897774312, 0.001463949792551393, 0.0014670584532224622, 0.0014668347166036657, 0.0014517565940686415, 0.0014534992369369252, 0.0014572955659224952, 0.0014538322606982216, 0.0014543582742881712, 0.0014642032528806618, 0.001443077758478274, 0.0014511801722475342, 0.001461173343862993, 0.0014881496524202437, 0.0014783961282042472, 0.0014825654748112497, 0.0014725338265188194, 0.0014755228018384406, 0.0014762098994274427, 0.0014676983815165945, 0.0014658388585863655, 0.0014756051929299186, 0.0014761110078153968, 0.0014817725793703142, 0.0014861614035040608, 0.0014808172552526681, 0.0014764167805475534, 0.001476876457437717, 0.0014768930480785528, 0.0014897440094225158, 0.0015049273835843354, 0.001496465189078612, 0.0014978338531784635, 0.0014982793782436499, 0.0015087082166612671, 0.001524599237463866, 0.001524657089414001, 0.0015239584643173697, 0.0015358446270446067, 0.00153162492587629, 0.0015397027479069282, 0.0015375088899746857, 0.0015290592566810852, 0.0015487047012031085, 0.001557741251978857, 0.0015743914469485813, 0.0015769652059703042, 0.0015742874359845594, 0.0015785992019977043, 0.0015760629379194724, 0.0015763666164347013, 0.001569959736538802, 0.001568653598202281, 0.001557804519236831, 0.0015584423550389149, 0.0015446326872448329, 0.001540084319194633, 0.0015324478571165064, 0.0015280465103591086, 0.0015348645081946347, 0.0015375083638538359, 0.0015398935298804183, 0.00151073616219736, 0.0015039153922662095, 0.0015030415790329788, 0.0015233010042813976, 0.0015319355839006692, 0.0015155401594689948, 0.0015064802025543287, 0.00150737616345456, 0.001510097844673792, 0.0015030314108881843, 0.0014915007885513449, 0.0014902454761786656, 0.001489500330832081, 0.0014798659365657277, 0.0014856334761534588, 0.0014727445534581983, 0.0014861827452259887, 0.0014844427503971174, 0.0014862676769254464, 0.0014905155020030559, 0.0014926701046287898, 0.0015035387226315499, 0.0014926568389234149, 0.0015057725577475687, 0.0014940681293475985, 0.0014880876250437133, 0.0014728638533932162, 0.0014709758791841808, 0.001468283435093823, 0.001458640020196171, 0.0014461727460595257, 0.0014382451813586884, 0.0014359525375553562, 0.0014290473326253933, 0.0014233983503603533, 0.0014238944134420317, 0.0014120168799274578, 0.0014039541874534822, 0.001407293749819998, 0.0014009054974639338, 0.001390733071003174, 0.0013792520410547172, 0.0013707110514643824, 0.0013824478730923527, 0.001380046676952019, 0.001382401364395425, 0.0013783449792397582, 0.0013734195333586251, 0.001372155559579869, 0.0013692513695401961, 0.0013713015941778036, 0.0013688596074766415, 0.0013746227182691016, 0.0013648462776223586, 0.001371102773917902, 0.0013514918208842109, 0.001356525170415047, 0.0013583816317838408, 0.0013638355820390405, 0.0013683885962626294, 0.0013726278124573895, 0.0013686045562209647, 0.0013791246858055463, 0.0013783933147086336, 0.0013791894501548243, 0.0013795538423352469, 0.0013718052979657263, 0.0013842780600789354, 0.0013862416862797438, 0.001394260711104626, 0.0013911570022919686, 0.001404590008313145, 0.0014108122855647342, 0.0014136250874243305, 0.0014183529281961676, 0.0014294919093009505, 0.0014301211247784115, 0.0014255588658089236, 0.0014343818639532965, 0.0014483420403803746, 0.0014518414145802158, 0.0014535529467113843, 0.0014662807393840407, 0.0014676684168920611, 0.0014755218521607528, 0.0014869348931889157, 0.0014892723318458742, 0.0014842113916989257, 0.0014896508550039304, 0.0014969671777919225, 0.0014946561431002504, 0.0014847563066960555, 0.0014960222768714683, 0.0015000269030514537, 0.001483268739425041, 0.0014795149458228535, 0.0014870500283502965, 0.0014939284675214238, 0.001510783728163002, 0.001501008250314242, 0.0015140978794897523, 0.001501095252993606, 0.001487273274896801, 0.0014811791691192846, 0.0014895524709944382, 0.0014844220671233561, 0.0014928125156413736, 0.0014939945072449646, 0.001497556152363445, 0.001500912458784002, 0.0015080389378743194, 0.0015205438757711059, 0.001517697587089136, 0.0015147981016592977, 0.0015085076615422147, 0.0015164535001234959, 0.0015227426624883028, 0.0015247858858105164, 0.0015206509946643358, 0.0015246720916330435, 0.001524272483491246, 0.0015373986200021056, 0.0015434966092027769, 0.0015498086829933066, 0.0015541770504246797, 0.0015642973568046818, 0.0015580145478504392, 0.0015596468156942218, 0.0015625304054395774, 0.0015612966607181252, 0.0015534913050655865, 0.0015474011229502866, 0.0015629288822274448, 0.0015589404934360416, 0.0015527326374571912, 0.001534596538999088, 0.0015236595860153726, 0.0015325560064059602, 0.001531072403535341, 0.0015211395956462566, 0.0015260639645183647, 0.0015288519323970575, 0.001526282038698865, 0.0015352228816941484, 0.0015329684313851917, 0.001534715868872077, 0.0015393469537343931, 0.0015439139401245484, 0.0015338547649041087, 0.0015320591146718934, 0.001531276364426877, 0.0015392016919607311, 0.0015387133642182143, 0.001543491175679844, 0.0015503202136940666, 0.0015587080288682674, 0.0015466170660390858, 0.001546235402587305, 0.0015625420800077108, 0.0015559561037204047, 0.0015614813753166538, 0.0015645504523664664, 0.0015672492905004435, 0.0015727424897873854, 0.0015878758372744582, 0.001589487128529414, 0.001592150969146353, 0.001578275648519723, 0.0015854360313303814, 0.001588617860127053, 0.0015905301005522598, 0.0015891033733821834, 0.0015818544511510084, 0.0015746863010313876, 0.001570357330932992, 0.001560064135259288, 0.0015635254104832182, 0.0015550631671730271, 0.001557848936676862, 0.0015571161662020288, 0.0015619437147615517, 0.0015611923906440617, 0.0015644122035555655, 0.001557009145398494, 0.0015519913581713625, 0.001551486168495963, 0.0015391835700921028, 0.0015500205333426297, 0.0015470299344430085, 0.0015570010163937103]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m lgbBO \u001b[38;5;241m=\u001b[39m BayesianOptimization(lgb_eval, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m4000\u001b[39m),\n\u001b[0;32m      2\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m63\u001b[39m),\n\u001b[0;32m      3\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_l2\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.05\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_data_in_leaf\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m      7\u001b[0m                                                 })\n\u001b[1;32m----> 9\u001b[0m lgbBO\u001b[38;5;241m.\u001b[39mmaximize(n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, init_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[0;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobe(x_probe, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:209\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mprobe(params)\n\u001b[1;32m--> 209\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\bayesian_optimization.py:62\u001b[0m, in \u001b[0;36mObservable.dispatch\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdispatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, event):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_subscribers(event)\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 62\u001b[0m         callback(event, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\logger.py:127\u001b[0m, in \u001b[0;36mScreenLogger.update\u001b[1;34m(self, event, instance)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m         colour \u001b[38;5;241m=\u001b[39m Colours\u001b[38;5;241m.\u001b[39mpurple \u001b[38;5;28;01mif\u001b[39;00m is_new_max \u001b[38;5;28;01melse\u001b[39;00m Colours\u001b[38;5;241m.\u001b[39mblack\n\u001b[1;32m--> 127\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step(instance, colour\u001b[38;5;241m=\u001b[39mcolour) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_END:\n\u001b[0;32m    129\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_header_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\logger.py:83\u001b[0m, in \u001b[0;36mScreenLogger._step\u001b[1;34m(self, instance, colour)\u001b[0m\n\u001b[0;32m     80\u001b[0m cells \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     82\u001b[0m cells\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_number(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 83\u001b[0m cells\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_number(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_constrained:\n\u001b[0;32m     85\u001b[0m     cells\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_bool(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallowed\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bayes_opt\\logger.py:42\u001b[0m, in \u001b[0;36mScreenLogger._format_number\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mx:<\u001b[39m\u001b[38;5;132;01m{s}\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m     39\u001b[0m             s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_cell_size,\n\u001b[0;32m     40\u001b[0m         )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mx:<\u001b[39m\u001b[38;5;132;01m{s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{p}\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     43\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[0;32m     44\u001b[0m         s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_cell_size,\n\u001b[0;32m     45\u001b[0m         p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_precision,\n\u001b[0;32m     46\u001b[0m     )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_cell_size:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m s:\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': None,\n",
       " 'params': {'lambda_l1': 0.016917080710503907,\n",
       "  'lambda_l2': 0.023312835928299858,\n",
       "  'max_depth': 18.822059380069007,\n",
       "  'min_child_samples': 7187.382871672484,\n",
       "  'min_data_in_leaf': 1378.9641699179717,\n",
       "  'num_leaves': 550.7421814182735}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': None,\n",
       " 'params': {'lambda_l1': 0.016917080710503907,\n",
       "  'lambda_l2': 0.023312835928299858,\n",
       "  'max_depth': 18.822059380069007,\n",
       "  'min_child_samples': 7187.382871672484,\n",
       "  'min_data_in_leaf': 1378.9641699179717,\n",
       "  'num_leaves': 550.7421814182735}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
